{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.2.0\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m794.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805910 sha256=700cf3597aece4d4d33985cff86ea722cb4828abc92d6a43191eda2f3b037411\n",
      "  Stored in directory: /home/majid/.cache/pip/wheels/32/97/d3/8b6d964c8700e4fbb561c71638a92ec55dac9be51eb5fea86d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install  findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set java home path if not set\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = 'xxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 3.2.0\n",
      "pythonVer 3.10\n",
      "master local\n",
      "spark home None\n",
      "user majid\n",
      "appname pyspark-shell\n",
      "appid local-1711400423228\n",
      "level of parallelism 1\n",
      "defaultMinPartitions 1\n"
     ]
    }
   ],
   "source": [
    "# inspect SparkContext\n",
    "print(\"version\",sc.version)   #Retrieve SparkContext version\n",
    "print(\"pythonVer\",sc.pythonVer) #Retrieve Python version\n",
    "print(\"master\",sc.master)  #Master URL to connect to\n",
    "print('spark home',str(sc.sparkHome))  #Path where Spark is installed on worker nodes\n",
    "print(\"user\",str(sc.sparkUser())) #Retrieve name of the Spark User running SparkContext\n",
    "print(\"appname\",sc.appName ) #Return application name\n",
    "print(\"appid\",sc.applicationId)  #Retrieve application ID\n",
    "print(\"level of parallelism\",sc.defaultParallelism) #Return default level of parallelism\n",
    "print(\"defaultMinPartitions\",sc.defaultMinPartitions) #Default minimum number of partitions forRDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    " .setMaster(\"local\")\n",
    " .setAppName(\"appname\" )\n",
    " .set(\"spark.executor.memory\" , \"1g\" ))\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized Collections\n",
    "# rdd is a Resilient Distributed Dataset\n",
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "rdd4 = sc.parallelize([(\"a\" ,[\"x\" ,\"y\" ,\"z\" ]),(\"b\" ,[\"p\" ,\"r\" ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List the number of partitions 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count RDD instances 3\n",
      "Count RDD instances by key defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"List the number of partitions\",rdd.getNumPartitions())\n",
    "print(\"Count RDD instances\",str(rdd.count()))\n",
    "print(\"Count RDD instances by key\",str(rdd.countByKey()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count RDD instances by value defaultdict(<class 'int'>, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})\n",
      "Return (key,value) pairs as a dictionary {'a': 2, 'b': 2}\n",
      "Sum of RDD elements 4950\n",
      "Check whether RDD is empty True\n"
     ]
    }
   ],
   "source": [
    "print(\"Count RDD instances by value\",str(rdd.countByValue()))\n",
    "print(\"Return (key,value) pairs as a dictionary\",str(rdd.collectAsMap()))\n",
    "print(\"Sum of RDD elements\",str(rdd3.sum()))\n",
    "print(\"Check whether RDD is empty\",str(sc.parallelize([]).isEmpty()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 99\n",
      "min 0\n",
      "mean 49.5\n",
      "stdev 28.86607004772212\n",
      "variance 833.25\n",
      "histogram ([0, 33, 66, 99], [33, 33, 34])\n",
      "stats (count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99, min: 0)\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "print('max',rdd3.max()) #Maximum value of RDD elements 99\n",
    "print('min',rdd3.min()) #Minimum value of RDD elements0\n",
    "print('mean',rdd3.mean()) #Mean value of RDD elements 49.5\n",
    "print('stdev',rdd3.stdev()) #Standard deviation of RDD elements 28.866070047722118\n",
    "print('variance',rdd3.variance()) #Compute variance of RDD elements 833.25\n",
    "print('histogram',rdd3.histogram(3)) #Compute histogram by bins([0,33,66,99],[33,33,34])\n",
    "print('stats',rdd3.stats()) #Summary statistics (count, mean, stdev, max & min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flat map is used to flatten the result\n",
    "# map is used to apply a function to each RDD element\n",
    "# collect is used to collect the result\n",
    "\n",
    "\n",
    "#Apply a function to each RDD element and flatten the result>>>\n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))\n",
    "rdd5.collect()\n",
    "#['a',7 , 7 ,  'a' , 'a' , 2,  2,  'a', 'b', 2 , 2, 'b']\n",
    "\n",
    "#Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
    "rdd4.flatMapValues(lambda x: x).collect()\n",
    "#[('a', 'x'), ('a', 'y'), ('a', 'z'),('b', 'p'),('b', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply a function to each RFD element\n",
    "rdd.map(lambda x: x +(x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = rdd.flatMap(lambda x: x +(x[1],x[0]))\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a list with all RDD elements [('a', 7), ('a', 2), ('b', 2)]\n",
      "Return the first element of the RDD ('a', 7)\n",
      "Return the first n elements of the RDD [('a', 7), ('a', 2)]\n",
      "Return the top n elements of the RDD [('b', 2), ('a', 7)]\n"
     ]
    }
   ],
   "source": [
    "# Getting\n",
    "print(\"Return a list with all RDD elements\",rdd.collect())\n",
    "print(\"Return the first element of the RDD\",rdd.first())\n",
    "print(\"Return the first n elements of the RDD\",rdd.take(2))\n",
    "print(\"Return the top n elements of the RDD\",rdd.top(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling\n",
    "rdd3.sample(False, 0.15, 81).collect() \n",
    "#Return sampled subset of rdd3     [3,4,27,31,40,41,42,43,60,76,79,80,86,97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering\n",
    "rdd.filter(lambda x: \"a\" in x).collect() #Filter the RDD[('a',7),('a',2)]\n",
    "rdd5.distinct().collect() #Return distinct RDD values['a' ,2, 'b',7]\n",
    "rdd.keys().collect() #Return (key,value) RDD's keys['a',  'a',  'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('a', 7)\n",
      "('a', 2)\n",
      "('b', 2)\n"
     ]
    }
   ],
   "source": [
    "def g (x): print(x)\n",
    "\n",
    "rdd.foreach(g) #Apply a function to all RDD elements('a', 7)('b', 2)('a', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reducing\n",
    "rdd.reduceByKey(lambda x,y : x+y).collect()\n",
    " #Merge the rdd values for each key[('a',9),('b',2)]\n",
    "\n",
    "rdd.reduce(lambda a, b: a+ b) #Merge the rdd values('a', 7, 'a' , 2 , 'b' , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [0,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   12,\n",
       "   14,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   22,\n",
       "   24,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   36,\n",
       "   38,\n",
       "   40,\n",
       "   42,\n",
       "   44,\n",
       "   46,\n",
       "   48,\n",
       "   50,\n",
       "   52,\n",
       "   54,\n",
       "   56,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   64,\n",
       "   66,\n",
       "   68,\n",
       "   70,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   82,\n",
       "   84,\n",
       "   86,\n",
       "   88,\n",
       "   90,\n",
       "   92,\n",
       "   94,\n",
       "   96,\n",
       "   98]),\n",
       " (1,\n",
       "  [1,\n",
       "   3,\n",
       "   5,\n",
       "   7,\n",
       "   9,\n",
       "   11,\n",
       "   13,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   21,\n",
       "   23,\n",
       "   25,\n",
       "   27,\n",
       "   29,\n",
       "   31,\n",
       "   33,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   41,\n",
       "   43,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   53,\n",
       "   55,\n",
       "   57,\n",
       "   59,\n",
       "   61,\n",
       "   63,\n",
       "   65,\n",
       "   67,\n",
       "   69,\n",
       "   71,\n",
       "   73,\n",
       "   75,\n",
       "   77,\n",
       "   79,\n",
       "   81,\n",
       "   83,\n",
       "   85,\n",
       "   87,\n",
       "   89,\n",
       "   91,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   99])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping by\n",
    "rdd3.groupBy(lambda x: x % 2) \\\n",
    ".mapValues(list).collect() #Return RDD of grouped values \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [7, 2]), ('b', [2])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey() \\\n",
    ".mapValues(list).collect()        \n",
    " # [('a',[7,2]),('b',[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (9, 2)), ('b', (2, 1))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregating\n",
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1))\n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "#Aggregate RDD elements of each partition and then the results\n",
    "rdd3.aggregate((0,0),seqOp,combOp) #(4950,100)\n",
    "#Aggregate values of each RDD key>>> \n",
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect()      #[('a',(9,2)), ('b',(2,1))]\n",
    "#Aggregate the elements of each partition, and then the results>>> rdd3.fold(0,add)     4950#Merge the values for each key>>> rdd.foldByKey(0, add).collect()[('a' ,9), ('b' ,2)]#Create tuples of RDD elements by applying a function>>> rdd3.keyBy(lambda x: x+x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd [('a', 7), ('a', 2), ('b', 2)]\n",
      "rdd2 [('a', 2), ('d', 1), ('b', 1)]\n",
      "substract rdd from rdd2 [('a', 7), ('b', 2)]\n",
      "subtractByKey [('d', 1)]\n",
      "cartesian [(('a', 7), ('a', 2)), (('a', 7), ('d', 1)), (('a', 7), ('b', 1)), (('a', 2), ('a', 2)), (('a', 2), ('d', 1)), (('a', 2), ('b', 1)), (('b', 2), ('a', 2)), (('b', 2), ('d', 1)), (('b', 2), ('b', 1))]\n"
     ]
    }
   ],
   "source": [
    "print('rdd',rdd.collect())\n",
    "print('rdd2',rdd2.collect())\n",
    "print('substract rdd from rdd2',rdd.subtract(rdd2).collect())\n",
    "rdd.subtract(rdd2).collect()\n",
    "#Return each rdd value not contained in rdd2[('b' ,2), ('a' ,7)]\n",
    "#Return each (key,value) pair of rdd2 with no matching key in rdd\n",
    "print('subtractByKey',rdd2.subtractByKey(rdd).collect())\n",
    "print('cartesian',rdd.cartesian(rdd2).collect()) #Return the Cartesian product of rdd and rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort by value [('d', 1), ('b', 1), ('a', 2)]\n",
      "sort by value [('a', 2), ('b', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('sort by value', rdd2.sortBy(lambda x: x[1]).collect())\n",
    " #Sort RDD by given function[('d',1),('b',1),('a',2)]\n",
    "print('sort by value', rdd2.sortByKey().collect() ) #Sort (key, value) ROD by key[('a' ,2), ('b' ,1), ('d' ,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[104] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(4) #New RDD with 4 partitions\n",
    "rdd.coalesce(1) #Decrease the number of partitions in the RDD to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd.saveAsTextFile(\"rdd.txt\") \n",
    "# rdd.saveAsHadoopFile(\"hdfs:// namenodehost/parent/child\",\n",
    "# 'org.apache.hadoop.mapred.TextOutputFormat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
