{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.2.0\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m794.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805910 sha256=700cf3597aece4d4d33985cff86ea722cb4828abc92d6a43191eda2f3b037411\n",
      "  Stored in directory: /home/majid/.cache/pip/wheels/32/97/d3/8b6d964c8700e4fbb561c71638a92ec55dac9be51eb5fea86d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install  findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 13:33:30 WARN Utils: Your hostname, majid resolves to a loopback address: 127.0.1.1; using 192.168.0.30 instead (on interface enp2s0)\n",
      "24/10/26 13:33:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/majid/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/26 13:33:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master ='local')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set java home path if not set\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = 'xxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 3.2.0\n",
      "pythonVer 3.10\n",
      "master local\n",
      "spark home None\n",
      "user majid\n",
      "appname pyspark-shell\n",
      "appid local-1729942421033\n",
      "level of parallelism 1\n",
      "defaultMinPartitions 1\n"
     ]
    }
   ],
   "source": [
    "# inspect SparkContext\n",
    "print(\"version\",sc.version)   #Retrieve SparkContext version\n",
    "print(\"pythonVer\",sc.pythonVer) #Retrieve Python version\n",
    "print(\"master\",sc.master)  #Master URL to connect to\n",
    "print('spark home',str(sc.sparkHome))  #Path where Spark is installed on worker nodes\n",
    "print(\"user\",str(sc.sparkUser())) #Retrieve name of the Spark User running SparkContext\n",
    "print(\"appname\",sc.appName ) #Return application name\n",
    "print(\"appid\",sc.applicationId)  #Retrieve application ID\n",
    "print(\"level of parallelism\",sc.defaultParallelism) #Return default level of parallelism\n",
    "print(\"defaultMinPartitions\",sc.defaultMinPartitions) #Default minimum number of partitions forRDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    " .setMaster(\"local\")\n",
    " .setAppName(\"appname\" )\n",
    " .set(\"spark.executor.memory\" , \"1g\" ))\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partitions in spark help to split the data into multiple parts and process them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized Collections\n",
    "# rdd is a Resilient Distributed Dataset\n",
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "rdd4 = sc.parallelize([(\"a\" ,[\"x\" ,\"y\" ,\"z\" ]),(\"b\" ,[\"p\" ,\"r\" ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List the number of partitions 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count RDD instances 3\n",
      "Count RDD instances by key defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"List the number of partitions\",rdd.getNumPartitions())\n",
    "print(\"Count RDD instances\",str(rdd.count()))\n",
    "print(\"Count RDD instances by key\",str(rdd.countByKey()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count RDD instances by value defaultdict(<class 'int'>, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})\n",
      "Return (key,value) pairs as a dictionary {'a': 2, 'b': 2}\n",
      "Sum of RDD elements 4950\n",
      "Check whether RDD is empty True\n"
     ]
    }
   ],
   "source": [
    "print(\"Count RDD instances by value\",str(rdd.countByValue()))\n",
    "print(\"Return (key,value) pairs as a dictionary\",str(rdd.collectAsMap()))\n",
    "print(\"Sum of RDD elements\",str(rdd3.sum()))\n",
    "print(\"Check whether RDD is empty\",str(sc.parallelize([]).isEmpty()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 99\n",
      "min 0\n",
      "mean 49.5\n",
      "stdev 28.86607004772212\n",
      "variance 833.25\n",
      "histogram ([0, 33, 66, 99], [33, 33, 34])\n",
      "stats (count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99, min: 0)\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "print('max',rdd3.max()) #Maximum value of RDD elements 99\n",
    "print('min',rdd3.min()) #Minimum value of RDD elements0\n",
    "print('mean',rdd3.mean()) #Mean value of RDD elements 49.5\n",
    "print('stdev',rdd3.stdev()) #Standard deviation of RDD elements 28.866070047722118\n",
    "print('variance',rdd3.variance()) #Compute variance of RDD elements 833.25\n",
    "print('histogram',rdd3.histogram(3)) #Compute histogram by bins([0,33,66,99],[33,33,34])\n",
    "print('stats',rdd3.stats()) #Summary statistics (count, mean, stdev, max & min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' map () example : returns squared numbers'''\n",
    "RDD = sc.parallelize ( [1,2,3,4])\n",
    "RDD.map (lambda x: x * x)\n",
    "\n",
    "''' filter () example : returns bigger than 2 '''\n",
    "RDD = sc.parallelize ( [1,2,3,4])\n",
    "RDD_filter = RDD.filter (lambda x: x > 2)\n",
    "\n",
    "''' flatMap () transformation example: retrun multiple values for each element splitted '''\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap (lambda x: x.split (\" \"))\n",
    "\n",
    "''' union () example : return warnings and errors as badlines or combined '''\n",
    "inputRDD = sc.textFile (\"Logs.txt\")\n",
    "errorRDD= inputRDD.filter (lambda x: \"error\" in x.split())\n",
    "warningsRDD=inputRDD.filter (lambda x: \"warnings\" in x.split())\n",
    "combinedRDD = errorRDD.union(warningsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flat map is used to flatten the result\n",
    "# map is used to apply a function to each RDD element\n",
    "# collect is used to collect the result\n",
    "\n",
    "\n",
    "#Apply a function to each RDD element and flatten the result>>>\n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))\n",
    "rdd5.collect()\n",
    "#['a',7 , 7 ,  'a' , 'a' , 2,  2,  'a', 'b', 2 , 2, 'b']\n",
    "\n",
    "#Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys\n",
    "rdd4.flatMapValues(lambda x: x).collect()\n",
    "#[('a', 'x'), ('a', 'y'), ('a', 'z'),('b', 'p'),('b', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply a function to each RFD element\n",
    "rdd.map(lambda x: x +(x[1],x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = rdd.flatMap(lambda x: x +(x[1],x[0]))\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a list with all RDD elements [('a', 7), ('a', 2), ('b', 2)]\n",
      "Return the first element of the RDD ('a', 7)\n",
      "Return the first n elements of the RDD [('a', 7), ('a', 2)]\n",
      "Return the top n elements of the RDD [('b', 2), ('a', 7)]\n"
     ]
    }
   ],
   "source": [
    "# Getting\n",
    "print(\"Return a list with all RDD elements\",rdd.collect())\n",
    "print(\"Return the first element of the RDD\",rdd.first())\n",
    "print(\"Return the first n elements of the RDD\",rdd.take(2))\n",
    "print(\"Return the top n elements of the RDD\",rdd.top(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling\n",
    "rdd3.sample(False, 0.15, 81).collect() \n",
    "#Return sampled subset of rdd3     [3,4,27,31,40,41,42,43,60,76,79,80,86,97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering\n",
    "rdd.filter(lambda x: \"a\" in x).collect() #Filter the RDD[('a',7),('a',2)]\n",
    "rdd5.distinct().collect() #Return distinct RDD values['a' ,2, 'b',7]\n",
    "rdd.keys().collect() #Return (key,value) RDD's keys['a',  'a',  'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('a', 7)\n",
      "('a', 2)\n",
      "('b', 2)\n"
     ]
    }
   ],
   "source": [
    "def g (x): print(x)\n",
    "\n",
    "rdd.foreach(g) #Apply a function to all RDD elements('a', 7)('b', 2)('a', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,3,4,6]\n",
    "RDD = sc.parallelize (x)\n",
    "''' aggregate Xs in list '''\n",
    "RDD.reduce(lambda x, y: x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [0,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   12,\n",
       "   14,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   22,\n",
       "   24,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   36,\n",
       "   38,\n",
       "   40,\n",
       "   42,\n",
       "   44,\n",
       "   46,\n",
       "   48,\n",
       "   50,\n",
       "   52,\n",
       "   54,\n",
       "   56,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   64,\n",
       "   66,\n",
       "   68,\n",
       "   70,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   82,\n",
       "   84,\n",
       "   86,\n",
       "   88,\n",
       "   90,\n",
       "   92,\n",
       "   94,\n",
       "   96,\n",
       "   98]),\n",
       " (1,\n",
       "  [1,\n",
       "   3,\n",
       "   5,\n",
       "   7,\n",
       "   9,\n",
       "   11,\n",
       "   13,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   21,\n",
       "   23,\n",
       "   25,\n",
       "   27,\n",
       "   29,\n",
       "   31,\n",
       "   33,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   41,\n",
       "   43,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   53,\n",
       "   55,\n",
       "   57,\n",
       "   59,\n",
       "   61,\n",
       "   63,\n",
       "   65,\n",
       "   67,\n",
       "   69,\n",
       "   71,\n",
       "   73,\n",
       "   75,\n",
       "   77,\n",
       "   79,\n",
       "   81,\n",
       "   83,\n",
       "   85,\n",
       "   87,\n",
       "   89,\n",
       "   91,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   99])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping by\n",
    "rdd3.groupBy(lambda x: x % 2) \\\n",
    ".mapValues(list).collect() #Return RDD of grouped values \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [7, 2]), ('b', [2])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey() \\\n",
    ".mapValues(list).collect()        \n",
    " # [('a',[7,2]),('b',[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (9, 2)), ('b', (2, 1))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregating\n",
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1))\n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "#Aggregate RDD elements of each partition and then the results\n",
    "rdd3.aggregate((0,0),seqOp,combOp) #(4950,100)\n",
    "#Aggregate values of each RDD key>>> \n",
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect()      #[('a',(9,2)), ('b',(2,1))]\n",
    "#Aggregate the elements of each partition, and then the results>>> rdd3.fold(0,add)     4950#Merge the values for each key>>> rdd.foldByKey(0, add).collect()[('a' ,9), ('b' ,2)]#Create tuples of RDD elements by applying a function>>> rdd3.keyBy(lambda x: x+x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd [('a', 7), ('a', 2), ('b', 2)]\n",
      "rdd2 [('a', 2), ('d', 1), ('b', 1)]\n",
      "substract rdd from rdd2 [('a', 7), ('b', 2)]\n",
      "subtractByKey [('d', 1)]\n",
      "cartesian [(('a', 7), ('a', 2)), (('a', 7), ('d', 1)), (('a', 7), ('b', 1)), (('a', 2), ('a', 2)), (('a', 2), ('d', 1)), (('a', 2), ('b', 1)), (('b', 2), ('a', 2)), (('b', 2), ('d', 1)), (('b', 2), ('b', 1))]\n"
     ]
    }
   ],
   "source": [
    "print('rdd',rdd.collect())\n",
    "print('rdd2',rdd2.collect())\n",
    "print('substract rdd from rdd2',rdd.subtract(rdd2).collect())\n",
    "rdd.subtract(rdd2).collect()\n",
    "#Return each rdd value not contained in rdd2[('b' ,2), ('a' ,7)]\n",
    "#Return each (key,value) pair of rdd2 with no matching key in rdd\n",
    "print('subtractByKey',rdd2.subtractByKey(rdd).collect())\n",
    "print('cartesian',rdd.cartesian(rdd2).collect()) #Return the Cartesian product of rdd and rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort by value [('d', 1), ('b', 1), ('a', 2)]\n",
      "sort by value [('a', 2), ('b', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('sort by value', rdd2.sortBy(lambda x: x[1]).collect())\n",
    " #Sort RDD by given function[('d',1),('b',1),('a',2)]\n",
    "print('sort by value', rdd2.sortByKey().collect() ) #Sort (key, value) ROD by key[('a' ,2), ('b' ,1), ('d' ,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoalescedRDD[104] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.repartition(4) #New RDD with 4 partitions\n",
    "rdd.coalesce(1) #Decrease the number of partitions in the RDD to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
    "\n",
    "''' result :\n",
    "\n",
    "Key 4 has 5 Counts\n",
    "Key 3 has 10 Counts\n",
    "Key 1 has 2 Counts\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd.saveAsTextFile(\"rdd.txt\") \n",
    "# rdd.saveAsHadoopFile(\"hdfs:// namenodehost/parent/child\",\n",
    "# 'org.apache.hadoop.mapred.TextOutputFormat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello', 1)\n",
      "('world', 1)\n",
      "('how', 1)\n",
      "('you', 1)\n"
     ]
    }
   ],
   "source": [
    "string = \"hello world how are you\"\n",
    "splitRDD = sc.parallelize(string.split(\" \"))\n",
    "\n",
    "stop_words = ['is', 'am', 'are', 'the', 'for', 'a', 'an', 'of', 'on', 'in', 'at']\n",
    "# Convert the words in lower case and remove stop words from the stop_words curated list\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\n",
    "# Print the resulting tuple\n",
    "for word in resultRDD.collect():\n",
    "  print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello', 1)\n",
      "('world', 1)\n",
      "('how', 1)\n",
      "('you', 1)\n",
      "hello,1\n",
      "world,1\n",
      "how,1\n",
      "you,1\n"
     ]
    }
   ],
   "source": [
    "# Display the first 10 words and their frequencies from the input RDD\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values from the input RDD\n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{},{}\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
