{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark classification algorithm\n",
    "\n",
    "|Algorithm | Description | \n",
    "|---|---|\n",
    "|Logistic Regression| Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/27 17:29:00 WARN Utils: Your hostname, majid resolves to a loopback address: 127.0.1.1; using 192.168.0.230 instead (on interface wlp3s0)\n",
      "24/03/27 17:29:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/majid/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/27 17:29:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ML\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"tmp/iris.data\" is a tab separated file\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType,TimestampType,FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sepal_length\", FloatType(), True),\n",
    "    StructField(\"sepal_width\", FloatType(), True),\n",
    "    StructField(\"petal_length\", FloatType(), True),\n",
    "    StructField(\"petal_width\", FloatType(), True),\n",
    "    StructField(\"class\", StringType(), True)\n",
    "])\n",
    "df = spark.read.csv(\"tmp/iris.data\", schema=schema, header=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          class|\n",
      "+---------------+\n",
      "| Iris-virginica|\n",
      "|    Iris-setosa|\n",
      "|Iris-versicolor|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find distinct values in the fruit_name column\n",
    "df.select('class').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column for the label\n",
    "from pyspark.sql.functions import when\n",
    "df = df.withColumn(\"label\", when(df[\"class\"] == \"Iris-setosa\", 0).when(df[\"class\"] == \"Iris-versicolor\", 1).otherwise(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|label|\n",
      "+------------+-----------+------------+-----------+-----------+-----+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|    0|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|    0|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|    0|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|    0|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|    0|\n",
      "+------------+-----------+------------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_length: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "features_col = df.columns[:-2]\n",
    "target_col = df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features_col, outputCol='features')\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|label|            features|\n",
      "+------------+-----------+------------+-----------+-----------+-----+--------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|    0|[5.09999990463256...|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|    0|[4.90000009536743...|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|    0|[4.69999980926513...|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|    0|[4.59999990463256...|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|    0|[5.0,3.5999999046...|\n",
      "+------------+-----------+------------+-----------+-----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply standard scaler to the features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
    "df_scaled = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+-----+--------------------+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      class|label|            features|     scaled_features|\n",
      "+------------+-----------+------------+-----------+-----------+-----+--------------------+--------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|    0|[5.09999990463256...|[6.15892840615775...|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|    0|[4.90000009536743...|[5.91740202781600...|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|    0|[4.69999980926513...|[5.67587507362994...|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|    0|[4.59999990463256...|[5.55511188445906...|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|    0|[5.0,3.5999999046...|[6.03816521698687...|\n",
      "+------------+-----------+------------+-----------+-----------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the logistic regression model\n",
    "lr = LogisticRegression(featuresCol='scaled_features',\n",
    "                        labelCol='label',\n",
    "                         maxIter=10,\n",
    "                         regParam=0.3,\n",
    "                         elasticNetParam=0.01,\n",
    "                         family='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "train, test = df_scaled.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = lr_model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    2|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    2|       1.0|    4|\n",
      "|    0|       0.0|   11|\n",
      "|    2|       2.0|   13|\n",
      "|    1|       1.0|   12|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(featuresCol='scaled_features',\n",
    "                 labelCol='label', \n",
    "                #  smoothing=1.0, \n",
    "                #  modelType='multinomial'\n",
    "                 )\n",
    "\n",
    "nb_model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = nb_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    2|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    2|       1.0|   10|\n",
      "|    0|       0.0|   11|\n",
      "|    2|       2.0|    7|\n",
      "|    1|       1.0|   12|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
